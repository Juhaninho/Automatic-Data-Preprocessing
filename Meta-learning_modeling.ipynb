{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.decomposition import PCA\n",
    "import statsmodels.formula.api as sm\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the data and setting the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('Melbourne_housing_FULL.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_column = 'Price'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "n_estimators = 2\n",
    "lof_n_neighbors = 20\n",
    "pca_n_components = 3\n",
    "n_random_subset = 5\n",
    "sl_backward_elimination = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform categorical variables to nominal variables\n",
    "le = LabelEncoder()\n",
    "data['Suburb'] = le.fit_transform(data['Suburb'])\n",
    "data['Address'] = le.fit_transform(data['Address'])\n",
    "data['Regionname'] = le.fit_transform([str(rn) for rn in data['Regionname']])\n",
    "data['CouncilArea'] = le.fit_transform([str(ca) for ca in data['CouncilArea']])\n",
    "data['SellerG'] = le.fit_transform(data['SellerG'])\n",
    "data['Type'] = le.fit_transform(data['Type'])\n",
    "data['Method'] = le.fit_transform(data['Method'])\n",
    "data['Date'] = [int(d.split('/')[2]) for d in data['Date']]\n",
    "\n",
    "nominal_columnnames = ['Suburb', 'Address', 'Regionname', 'CouncilArea', 'SellerG', 'Type', 'Method', 'Date']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definition of the methods for datapreprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# handling missing values\n",
    "# drop rows with missing values\n",
    "def drop_rows(data_train):\n",
    "    return data_train.dropna()\n",
    "\n",
    "# replace missing values with zero\n",
    "def replace_with_zero(data_train):\n",
    "    return data_train.fillna(0)\n",
    "\n",
    "# replace missing values with mean\n",
    "def replace_with_mean(data_train):\n",
    "    return data_train.fillna(data_train.mean())\n",
    "\n",
    "# replace missing values with median\n",
    "def replace_with_median(data_train):\n",
    "    return data_train.fillna(data_train.median())\n",
    "\n",
    "# replace missing values with min\n",
    "def replace_with_min(data_train):\n",
    "    return data_train.fillna(data_train.min())\n",
    "\n",
    "# replace missing values with max\n",
    "def replace_with_max(data_train):\n",
    "    return data_train.fillna(data_train.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outliers\n",
    "# no outlier handling\n",
    "def no_outlier_handling(data_train):\n",
    "    return data_train\n",
    "\n",
    "# remove outliers with Interquartile Range\n",
    "def interquartile_range(data_train):\n",
    "    q25_train = np.percentile(data_train[target_column], 25)\n",
    "    q75_train = np.percentile(data_train[target_column], 75)\n",
    "    iqr_train = data_train.drop(data_train[(data_train[target_column] < q25_train) | \n",
    "                                           (data_train[target_column] > q75_train)].index, axis=0)\n",
    "    return iqr_train\n",
    "\n",
    "# remove outliers with BaggedLOF\n",
    "def bagged_lof(data_train):\n",
    "    lof = LocalOutlierFactor(n_neighbors=lof_n_neighbors, contamination=0.1)\n",
    "    outlier_train_pred = lof.fit_predict(data_train)\n",
    "    lof_train = data_train.drop(data_train[outlier_train_pred == -1].index, axis=0)\n",
    "    return lof_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformation\n",
    "# no transformation\n",
    "def no_transformation(data_train):\n",
    "    return data_train\n",
    "\n",
    "# normalization\n",
    "def normalization(data_train):\n",
    "    normalizer = preprocessing.Normalizer(norm='l2')\n",
    "    X_train = data_train.drop([target_column], axis=1)\n",
    "    y_train = data_train[target_column]\n",
    "    normalized_X_train = pd.DataFrame(normalizer.fit_transform(X_train), columns=X_train.columns, index=X_train.index)\n",
    "    normalized_train = pd.concat([normalized_X_train, y_train], axis=1)\n",
    "    return normalized_train\n",
    "\n",
    "# standardize\n",
    "def standardization(data_train):\n",
    "    standardizer = preprocessing.StandardScaler()\n",
    "    X_train = data_train.drop([target_column], axis=1)\n",
    "    y_train = data_train[target_column]\n",
    "    standardized_X_train = pd.DataFrame(standardizer.fit_transform(X_train), columns=X_train.columns, index=X_train.index)\n",
    "    standardized_train = pd.concat([standardized_X_train, y_train], axis=1)\n",
    "    return standardized_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimensionality Reduction\n",
    "# no Dimensionality Reduction\n",
    "def no_reduction(data_train):\n",
    "    return data_train\n",
    "\n",
    "# PCA\n",
    "def pca(data_train):\n",
    "    pca = PCA(n_components=pca_n_components)\n",
    "    X_train = data_train.drop([target_column], axis=1)\n",
    "    y_train = data_train[target_column]\n",
    "    pca_X_train = pd.DataFrame(data=pca.fit_transform(X_train), index=X_train.index)\n",
    "    pca_train = pd.concat([pca_X_train, y_train], axis=1)\n",
    "    return pca_train\n",
    "    \n",
    "# RandomSubset\n",
    "def random_subset(data_train):\n",
    "    X_train = data_train.drop([target_column], axis=1)\n",
    "    y_train = data_train[target_column]\n",
    "    random_X_train = X_train.sample(n_random_subset, axis=1)\n",
    "    random_train = pd.concat([random_X_train, y_train], axis=1)\n",
    "    return random_train\n",
    "\n",
    "# Backward Elimination\n",
    "def backward_elimination(data_train):\n",
    "    X_train = data_train.drop([target_column], axis=1)\n",
    "    y_train = data_train[target_column]\n",
    "    numVars = X_train.shape[1]\n",
    "    for i in range(0, numVars):\n",
    "        regressor_OLS = sm.OLS(endog = y_train, exog = X_train).fit()\n",
    "        maxVar = max(regressor_OLS.pvalues)\n",
    "        if maxVar > sl_backward_elimination:\n",
    "            for j in range(0, numVars - i):\n",
    "                if (regressor_OLS.pvalues[j] == maxVar):\n",
    "                    X_train.drop([X_train.columns[j]], axis=1, inplace=True)\n",
    "    be_train = pd.concat([X_train, y_train], axis=1)\n",
    "    return be_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Methods to building the meta-model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_model = pd.DataFrame(columns=['number_of_instances', 'number_of_features', 'noise_to_signal_ratio', \n",
    "           'mean_kurtosis_of_numeric_attributes', 'mean_means_of_numeric_attributes', \n",
    "           'mean_std_dev_of_numeric_attributes', 'mean_mutual_information', 'max_nominal_att_distinct_values', \n",
    "           'mean_std_dev_nominal_att_distinct_values', 'mean_nominal_att_distinct_values', \n",
    "           'mean_skewness_of_numeric_attributes', 'prediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = data.shape[1] - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adj_r2(y_true, y_pred):\n",
    "    global p\n",
    "    R2 = metrics.r2_score(y_true, y_pred)\n",
    "    n = len(y_true)\n",
    "    return 1-(1-R2)*(n-1)/(n-p-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(dataset_train):\n",
    "    X, y = getXy(dataset_train)\n",
    "    p = X.shape[1]\n",
    "    regressor = DecisionTreeRegressor(random_state=0)\n",
    "    cv_results = cross_validate(regressor, X, y, cv=5, scoring=metrics.make_scorer(adj_r2)) # cv=len(dataset_train)\n",
    "    return cv_results['test_score'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_to_metadatabase(dataset):\n",
    "    metadata = list(getMetadata(dataset))\n",
    "    prediction = predict(dataset)\n",
    "    metadata.append(prediction)\n",
    "    data_model.loc[len(data_model)] = metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_meta_model():\n",
    "    # Grid-search to build the meta-database for the model\n",
    "    handling_missing_values = [drop_rows, replace_with_zero, replace_with_mean, replace_with_median, replace_with_min, replace_with_max] #\n",
    "    handling_outliers = [no_outlier_handling, interquartile_range, bagged_lof] #\n",
    "    transformations = [no_transformation, normalization, standardization] #\n",
    "    dimensionality_reduction = [no_reduction, pca, random_subset, backward_elimination] #\n",
    "    for missing_values_method in handling_missing_values:\n",
    "        filled_data = missing_values_method(data)\n",
    "        print(missing_values_method.__name__)\n",
    "        for outlier_method in handling_outliers:\n",
    "            outliers_handled_data = outlier_method(filled_data)\n",
    "            for transformation_method in transformations:\n",
    "                transformed_data = transformation_method(outliers_handled_data)\n",
    "                for dim_reduction_method in dimensionality_reduction:\n",
    "                    if dim_reduction_method.__name__ == 'pca' and transformation_method.__name__ == 'no_transformation':\n",
    "                        continue\n",
    "                    reduced_data = dim_reduction_method(transformed_data)\n",
    "                    if reduced_data.shape[1] > 1:\n",
    "                        add_to_metadatabase(reduced_data)\n",
    "    #build regression-tree-model from meta-database\n",
    "    data_model.fillna(0, inplace=True)\n",
    "    regressor = DecisionTreeRegressor(random_state=1)\n",
    "    regressor.fit(data_model.drop(['prediction'], axis=1), data_model['prediction'])\n",
    "    return regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMetadata(dataset):\n",
    "    nominal_attributes, numeric_attributes = getNumericAndNominal(dataset)\n",
    "    number_of_instances = dataset.shape[0]\n",
    "    number_of_features = dataset.shape[1]\n",
    "    noise_to_signal_ratio = np.mean(dataset.mean(numeric_only=True)) / np.mean(dataset.std(numeric_only=True))\n",
    "    mean_kurtosis_of_numeric_attributes = np.mean(numeric_attributes.kurtosis(numeric_only=True))\n",
    "    mean_means_of_numeric_attributes = np.mean(numeric_attributes.mean(numeric_only=True))\n",
    "    mean_std_dev_of_numeric_attributes = np.mean(numeric_attributes.std(numeric_only=True))\n",
    "    mean_mutual_information = np.mean(mutual_info_regression(dataset.drop([target_column], axis=1), dataset[target_column]))\n",
    "    max_nominal_att_distinct_values = np.max(nominal_attributes.nunique())\n",
    "    mean_std_dev_nominal_att_distinct_values = np.mean(nominal_attributes.std())\n",
    "    mean_nominal_att_distinct_values = np.mean(nominal_attributes.nunique())\n",
    "    mean_skewness_of_numeric_attributes = np.mean(dataset.skew(numeric_only=True))\n",
    "    return [number_of_instances, number_of_features, noise_to_signal_ratio, mean_kurtosis_of_numeric_attributes, \n",
    "           mean_means_of_numeric_attributes, mean_std_dev_of_numeric_attributes, mean_mutual_information, \n",
    "           max_nominal_att_distinct_values, mean_std_dev_nominal_att_distinct_values, mean_nominal_att_distinct_values, \n",
    "           mean_skewness_of_numeric_attributes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getXy(dataset):\n",
    "    X_data = dataset.drop([target_column], axis=1)\n",
    "    y_data = dataset[target_column]\n",
    "    return X_data, y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNumericAndNominal(dataset):\n",
    "    nominal_att = dataset.loc[:, dataset.columns.isin(nominal_columnnames)]\n",
    "    numeric_att = dataset.drop(nominal_columnnames, axis=1, errors='ignore')\n",
    "    return nominal_att, numeric_att"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drop_rows\n",
      "replace_with_zero\n",
      "replace_with_mean\n",
      "replace_with_median\n",
      "replace_with_min\n",
      "replace_with_max\n"
     ]
    }
   ],
   "source": [
    "#building the meta-model\n",
    "meta_model = build_meta_model()\n",
    "data_model.to_csv('meta_model.csv', sep=',', encoding='utf-8', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
