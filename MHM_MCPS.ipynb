{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.utils import resample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the data and setting hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('Melbourne_housing_FULL.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_column = 'Price'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "lof_n_neighbors = 20\n",
    "pca_n_components = 3\n",
    "n_random_subset = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop([target_column], axis=1)\n",
    "y = data[target_column]\n",
    "\n",
    "# transform categorical variables to numerical variables\n",
    "le = LabelEncoder()\n",
    "X['Suburb'] = le.fit_transform(X['Suburb'])\n",
    "X['Address'] = le.fit_transform(X['Address'])\n",
    "X['Regionname'] = le.fit_transform([str(rn) for rn in X['Regionname']])\n",
    "X['CouncilArea'] = le.fit_transform([str(ca) for ca in X['CouncilArea']])\n",
    "X['SellerG'] = le.fit_transform(X['SellerG'])\n",
    "X['Type'] = le.fit_transform(X['Type'])\n",
    "X['Method'] = le.fit_transform(X['Method'])\n",
    "X['Date'] = [d.split('/')[2] for d in X['Date']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choosing of the methods for datapreprocessing automatically by MCPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train-test-split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=8)\n",
    "data_train = pd.concat([X_train, y_train], axis=1)\n",
    "data_test = pd.concat([X_test, y_test], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict the accuracy of the preprocessed training dataset with a 10-fold cross-validation\n",
    "def predict_accuracy(dataset_train):\n",
    "    X, y = getXy(dataset_train)\n",
    "    cv_results = cross_validate(DecisionTreeRegressor(random_state=0), X, y, cv=10, scoring=('r2'))\n",
    "    return cv_results['test_score'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#variables\n",
    "best_score = 0\n",
    "best_method = ''\n",
    "best_train_data = data_train.copy()\n",
    "mcps = []\n",
    "all_scores = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_score():\n",
    "    global best_score, best_method\n",
    "    best_score = 0\n",
    "    best_method = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_score(dataset_train, method):\n",
    "    global best_score, best_method, best_train_data, all_scores\n",
    "    score = predict_accuracy(dataset_train)\n",
    "    all_scores[method] = score\n",
    "    if score > best_score:\n",
    "        best_score = score\n",
    "        best_method = method\n",
    "        best_train_data = dataset_train.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getXy(dataset):\n",
    "    X_data = dataset.drop([target_column], axis=1)\n",
    "    y_data = dataset[target_column]\n",
    "    return X_data, y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# handling missing values\n",
    "init_score()\n",
    "data_train = best_train_data.copy()\n",
    "\n",
    "# drop rows with missing values\n",
    "update_score(data_train.dropna(), 'drop rows')\n",
    "\n",
    "# replace missing values with zero\n",
    "update_score(data_train.fillna(0), 'replace with zero')\n",
    "\n",
    "# replace missing values with mean\n",
    "update_score(data_train.fillna(data_train.mean()), 'replace with mean')\n",
    "\n",
    "# replace missing values with median\n",
    "update_score(data_train.fillna(data_train.median()), 'replace with median')\n",
    "\n",
    "# replace missing values with min\n",
    "update_score(data_train.fillna(data_train.min()), 'replace with min')\n",
    "\n",
    "# replace missing values with max\n",
    "update_score(data_train.fillna(data_train.max()), 'replace with max')\n",
    "\n",
    "filled_train_data = best_train_data.copy()\n",
    "mcps.append(best_method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outliers\n",
    "init_score()\n",
    "data_train = best_train_data.copy()\n",
    "\n",
    "# no outlier handling\n",
    "update_score(data_train, 'no outlier handling')\n",
    "\n",
    "# remove outliers with Interquartile Range\n",
    "q25_train = np.percentile(data_train[target_column], 25)\n",
    "q75_train = np.percentile(data_train[target_column], 75)\n",
    "iqr_train = data_train.drop(data_train[(data_train[target_column] < q25_train) | \n",
    "                                       (data_train[target_column] > q75_train)].index, axis=0)\n",
    "update_score(iqr_train, 'Interquartile Range')\n",
    "\n",
    "# remove outliers with BaggedLOF\n",
    "lof = LocalOutlierFactor(n_neighbors=lof_n_neighbors, contamination=0.1)\n",
    "outlier_train_pred = lof.fit_predict(data_train)\n",
    "lof_train = data_train.drop(data_train[outlier_train_pred == -1].index, axis=0)\n",
    "update_score(lof_train, 'BaggedLOF')\n",
    "\n",
    "outliers_handled_train_data = best_train_data.copy()\n",
    "mcps.append(best_method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformation\n",
    "init_score()\n",
    "data_train = best_train_data.copy()\n",
    "\n",
    "# no transformation\n",
    "update_score(data_train, 'no transformation')\n",
    "\n",
    "# normalization\n",
    "normalizer = preprocessing.Normalizer(norm='l2')\n",
    "X_train = data_train.drop([target_column], axis=1)\n",
    "y_train = data_train[target_column]\n",
    "normalized_X_train = pd.DataFrame(data=normalizer.fit_transform(X_train), columns=X_train.columns, index=X_train.index)\n",
    "normalized_train = pd.concat([normalized_X_train, y_train], axis=1)\n",
    "update_score(normalized_train, 'normalization')\n",
    "\n",
    "# standardize\n",
    "standardizer = preprocessing.StandardScaler()\n",
    "X_train = data_train.drop([target_column], axis=1)\n",
    "y_train = data_train[target_column]\n",
    "standardized_X_train = pd.DataFrame(data=standardizer.fit_transform(X_train), columns=X_train.columns, index=X_train.index)\n",
    "standardized_train = pd.concat([standardized_X_train, y_train], axis=1)\n",
    "update_score(standardized_train, 'standardization')\n",
    "\n",
    "transformed_train_data = best_train_data.copy()\n",
    "mcps.append(best_method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimensionality Reduction\n",
    "init_score()\n",
    "data_train = best_train_data.copy()\n",
    "\n",
    "# no Dimensionality Reduction\n",
    "update_score(data_train, 'no reduction')\n",
    "\n",
    "# PCA\n",
    "if 'standardization' in mcps or 'normalization' in mcps:\n",
    "    pca = PCA(n_components=pca_n_components)\n",
    "    X_train = data_train.drop([target_column], axis=1)\n",
    "    y_train = data_train[target_column]\n",
    "    pca_X_train = pd.DataFrame(data=pca.fit_transform(X_train), index=X_train.index)\n",
    "    pca_train = pd.concat([pca_X_train, y_train], axis=1)\n",
    "    update_score(pca_train, 'pca')\n",
    "else:\n",
    "    normalizer = preprocessing.Normalizer(norm='l2')\n",
    "    X_train = data_train.drop([target_column], axis=1)\n",
    "    y_train = data_train[target_column]\n",
    "    normalized_X_train = pd.DataFrame(data=normalizer.fit_transform(X_train), columns=X_train.columns, index=X_train.index)\n",
    "    pca = PCA(n_components=pca_n_components)\n",
    "    pca_X_train = pd.DataFrame(data=pca.fit_transform(normalized_X_train), index=normalized_X_train.index)\n",
    "    pca_train = pd.concat([pca_X_train, y_train], axis=1)\n",
    "    update_score(pca_train, 'pca')\n",
    "    \n",
    "    \n",
    "# RandomSubset\n",
    "X_train = data_train.drop([target_column], axis=1)\n",
    "y_train = data_train[target_column]\n",
    "random_X_train = X_train.sample(n_random_subset, axis=1)\n",
    "random_train = pd.concat([random_X_train, y_train], axis=1)\n",
    "update_score(random_train, 'random subset')\n",
    "\n",
    "reduced_train_data = best_train_data.copy()\n",
    "mcps.append(best_method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampling\n",
    "init_score()\n",
    "data_train = best_train_data.copy()\n",
    "\n",
    "# no sampling\n",
    "update_score(data_train, 'no sampling')\n",
    "\n",
    "# resampling\n",
    "resampled_train = resample(data_train, random_state=0)\n",
    "#update_score(resampled_train, 'resampling')\n",
    "\n",
    "sampled_train_data = best_train_data.copy()\n",
    "mcps.append(best_method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['drop rows', 'BaggedLOF', 'standardization', 'no reduction', 'no sampling']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mcps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.65844556082184735"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_score = best_score\n",
    "training_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'BaggedLOF': 0.65654237400252369,\n",
       " 'Interquartile Range': 0.20179587793229831,\n",
       " 'drop rows': 0.55010413872021025,\n",
       " 'no outlier handling': 0.55010413872021025,\n",
       " 'no reduction': 0.65844556082184735,\n",
       " 'no sampling': 0.65844556082184735,\n",
       " 'no transformation': 0.65654237400252369,\n",
       " 'normalization': 0.39209854369079211,\n",
       " 'pca': 0.2670175646113101,\n",
       " 'random subset': -0.17293456787528497,\n",
       " 'replace with max': -0.28301151717654333,\n",
       " 'replace with mean': 0.082972890387454201,\n",
       " 'replace with median': 0.012429201119353939,\n",
       " 'replace with min': -0.26632788118824335,\n",
       " 'replace with zero': -0.30557042870737516,\n",
       " 'standardization': 0.65844556082184735}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Application of the preprocessed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prediction(dataset_train, dataset_test):\n",
    "    X_train, y_train = getXy(dataset_train)\n",
    "    X_test, y_test = getXy(dataset_test)\n",
    "    regressor = DecisionTreeRegressor(random_state=0)\n",
    "    regressor.fit(X_train, y_train)\n",
    "    pred = regressor.predict(X_test)\n",
    "    return metrics.r2_score(y_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test.dropna(inplace=True)\n",
    "if 'standardization' in mcps:\n",
    "    standardizer = preprocessing.StandardScaler()\n",
    "    X_test = data_test.drop([target_column], axis=1)\n",
    "    y_test = data_test[target_column]\n",
    "    standardized_X_test = pd.DataFrame(standardizer.fit_transform(X_test), columns=X_test.columns, index=X_test.index)\n",
    "    data_test = pd.concat([standardized_X_test, y_test], axis=1)\n",
    "if 'normalization' in mcps:\n",
    "    normalizer = preprocessing.Normalizer(norm='l2')\n",
    "    X_test = data_test.drop([target_column], axis=1)\n",
    "    y_test = data_test[target_column]\n",
    "    normalized_X_test = pd.DataFrame(normalizer.fit_transform(X_test), columns=X_test.columns, index=X_test.index)\n",
    "    data_test = pd.concat([normalized_X_test, y_test], axis=1)\n",
    "if 'random subset' in mcps:\n",
    "    X_test = data_test.drop([target_column], axis=1)\n",
    "    y_test = data_test[target_column]\n",
    "    random_X_test = X_test[reduced_train_data.columns.drop([target_column])]\n",
    "    data_test = pd.concat([random_X_test, y_test], axis=1)\n",
    "if 'pca' in mcps:\n",
    "    if 'standardization' in mcps or 'normalization' in mcps:\n",
    "        pca = PCA(n_components=pca_n_components)\n",
    "        X_test = data_test.drop([target_column], axis=1)\n",
    "        y_test = data_test[target_column]\n",
    "        principal_components_test = pca.fit_transform(X_test)\n",
    "        pca_X_test = pd.DataFrame(data=principal_components_test)\n",
    "        data_test = pd.concat([pca_X_test, y_test], axis=1)\n",
    "    else:\n",
    "        normalizer = preprocessing.Normalizer(norm='l2')\n",
    "        normalized_test = pd.DataFrame(normalizer.fit_transform(data_test), columns=data_test.columns)\n",
    "        X_normalized_test = normalized_test.drop([target_column], axis=1)\n",
    "        y_normalized_test = normalized_test[target_column]\n",
    "        pca = PCA(n_components=pca_n_components)\n",
    "        principal_components_test = pca.fit_transform(X_normalized_test)\n",
    "        pca_X_test = pd.DataFrame(data=principal_components_test)\n",
    "        data_test = pd.concat([pca_X_test, y_normalized_test], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.60501118978640744"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_score = make_prediction(best_train_data, data_test)\n",
    "test_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
